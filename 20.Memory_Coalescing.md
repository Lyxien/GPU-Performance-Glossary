# 合并访存 （Memory Coalescing）	

内存合并（Memory Coalescing） 是一种硬件技术，通过在一次物理内存访问中处理多个逻辑内存读操作，从而提高内存带宽的利用率。

内存合并通常发生在 全局内存（global memory） 访问时。对于共享内存（shared memory）的高效访问，可以参考 bank conflict 相关内容。

在 CUDA GPU 中，全局内存由 GPU RAM 支持，这些 RAM 使用 DRAM 技术（如 GDDR 或 HBM）制造。DRAM 技术具有 高内存带宽，但访问延迟也很长（即使相比 CPU RAM 使用的 DDR5 技术也很高）。DRAM 的访问延迟受限于小电容给访问线路充电的速度，这在本质上受到热、功耗和尺寸的限制。由于延迟较高，如果每个逻辑内存访问都作为独立的物理访问处理，GPU 的内存带宽将无法被充分利用。

内存合并 利用 DRAM 技术的内部特性，使特定访问模式能够充分使用带宽。每次访问 DRAM 地址时，会将多个连续地址并行地一次性获取（单个时钟周期内）。

更详细的信息可以参考《Programming Massively Parallel Processors》第四版第 6.1 节；更全面的讲解见 Ulrich Drepper 的文章 What Every Programmer Should Know About Memory。

对这些连续内存位置的访问和传输称为 DRAM burst。如果多个并发逻辑访问通过一次物理 burst 被处理，则称该访问为 coalesced（合并访问）。注意，物理访问是内存事务（memory transaction）的一部分，这是内存合并中常用的术语。

在 CPU 上，将 burst 映射到缓存行（cache line）也能提高访问效率。与 GPU 编程中常见的情况类似，CPU 中的自动缓存行为在 GPU 中由程序员管理。

这其实并不难实现，因为 DRAM burst 与 CUDA PTX 的 单指令、多线程（SIMT） 执行模型高度契合。也就是说，在正常执行时，warp 内的所有线程同时执行同一条指令。这使得 CUDA 程序员容易编写 合并访问（coalesced access） 的程序，同时内存管理硬件也能轻松检测哪些访问可以被合并。通常，一次 burst 可以处理 128 字节——正好足够 warp 内的 32 个线程各加载一个 32 位浮点数（float）。

为了演示内存合并对性能的影响，考虑如下 kernel，它从数组中以可变步幅（stride，即访问元素间距）读取数据。随着步幅增大，每个 warp 所需的 DRAM burst 数量增加，导致每个逻辑访问需要更多物理访问，从而降低内存吞吐量。

```cpp
__global__ void strided_read_kernel(const float* __restrict__ in,
                                    float* __restrict__ out,
                                    size_t N, int stride)
{
    const size_t t  = blockIdx.x * blockDim.x + threadIdx.x;
    const size_t T  = gridDim.x * (size_t)blockDim.x;

    float acc = 0.f;

    for (size_t j = (size_t)t * (size_t)stride; j < N; j += (size_t)T * (size_t)stride) {
        // across a warp, addresses differ by (stride * sizeof(float))
        float v = in[j]; // perfectly coalesced for stride == 1
        acc = acc * 1.000000119f + v;  // force compiler to keep the load
    }

    // do one write per thread (negligible vs reads)
    if (t < N) out[t] = acc;
}
```

当我们在 Godbolt 上通过微基准测试（micro-benchmark）运行这个 kernel（你可以在这里复现）时，可以观察到 步幅（stride）与吞吐量（throughput）之间的预期关系：

```bash
# 设备: Tesla T4 (SM 75)
# N = 67108864 floats (256.0 MB), 迭代次数 = 10
stride        GB/s
    1       206.0
    2       130.5
    4        68.8
    8        33.8
   16        16.8
   32        15.2
   64        13.6
  128        11.2
```
也就是说，当步幅增加到 2 时，吞吐量几乎减半，因为每个 warp 的请求所需的 DRAM burst 数量翻倍。将步幅再翻倍到 4，吞吐量再次减半。当步幅达到 16 时，吞吐量降低了约 16 倍，此后的性能下降模式有所不同，这可能是由于其他内存子系统组件开始显现影响，同时由于数据局部性降低（例如设备上的 TLB 未命中），性能进一步下降。
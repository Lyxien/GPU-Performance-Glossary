# 计算瓶颈（Compute-bound）

## 计算瓶颈的 Kernel
计算受限的内核具有高算术强度（arithmetic intensity），也就是每加载或存储 1 字节的内存所执行的算术操作数很多。

在这种情况下，算术流水线（arithmetic pipes）的利用率是性能的主要限制因素。

## 举例：大型模型推理

大型扩散模型（diffusion model）的推理负载通常是 计算受限 的。

大语言模型（LLM）推理过程在批量预填充 / 提示处理（batch prefill / prompt processing）阶段也常常是计算受限的，因为在这个阶段中，每个权重（weight）可以加载到共享内存（shared memory）一次，然后被多个 token 重复使用。

### 简单计算例子

问题：
- 对于一个 计算受限的 Transformer 语言模型 推理，生成相邻两个 token 之间的最小延迟（inter-token latency，即每个输出 token 的时间）。

模型和硬件信息：
- 假设模型有 5000 亿个参数（500B parameters），以 16 位精度存储，总大小约为 1 TB。
- 运行在一块算术带宽（arithmetic bandwidth）为 1 PFLOP/s（一千万亿次浮点运算/秒） 的 GPU 上

计算过程：
- 该模型对每个 batch 元素（batch element）大约会执行 1 万亿次浮点运算（1T FLOPs）（Transformer 语言模型，每个参数执行一次乘法和一次加法，500B * 2 = 1T）。
- 理论上在计算受限的情况下（假设内存带宽等都不是瓶颈），每个 batch 元素生成一个 token 的 **最小延迟** 就是 1 毫秒（ 1T / 1P = $10^{12} / 10^{15}$ = $10^{-3}$ s = 1ms ）
- 如果延迟超过 1ms，就说明存在别的瓶颈
    - GPU 算力没被完全利用
    - 性能被其他因素“卡住”了，如：
        - 内存内存带宽不足， GPU 等待从显存取数据（Memory-bound）
        - 访存延迟大，cache 未命中或数据搬运耗时（Latency bottleneck）
        - 算子调用/调度开销，kernel launch 频繁（Overhead-bound）
        - 负载不均衡，有的 SM 空闲、有的超载（Occupancy 不高）
        - 通信瓶颈（多卡），等待 AllReduce / 同步（Network-bound）

补充：
- 若要在 batch size = 1 时让这块 GPU 真正处于计算受限状态，它必须具备 1 PB/s 的内存带宽（memory bandwidth），这样才能在 1 毫秒内加载完所有 1 TB 的权重
- 当前 GPU 的内存带宽普遍在 TB/s 级别，所以当前推理中通常需要成百上千个输入同时批处理（batch）来提供足够的算术强度，使得计算单元充分利用，从而真正达到“计算受限”。

